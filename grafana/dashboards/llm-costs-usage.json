{
  "__inputs": [
    {
      "name": "DS_PROMETHEUS",
      "label": "Prometheus",
      "description": "",
      "type": "datasource",
      "pluginId": "prometheus",
      "pluginName": "Prometheus"
    }
  ],
  "__requires": [
    { "type": "grafana", "id": "grafana", "name": "Grafana", "version": "10.0.0" },
    { "type": "datasource", "id": "prometheus", "name": "Prometheus", "version": "1.0.0" },
    { "type": "panel", "id": "timeseries", "name": "Time series", "version": "" },
    { "type": "panel", "id": "stat", "name": "Stat", "version": "" },
    { "type": "panel", "id": "piechart", "name": "Pie chart", "version": "" }
  ],
  "id": null,
  "uid": "quickgithub-llm",
  "title": "QuickGitHub - LLM Costs & Usage",
  "description": "LLM token usage, costs, latency, and call breakdown by model and stage",
  "tags": ["quickgithub", "llm", "otel", "genai"],
  "timezone": "browser",
  "editable": true,
  "graphTooltip": 1,
  "time": { "from": "now-6h", "to": "now" },
  "refresh": "30s",
  "templating": {
    "list": [
      {
        "name": "DS_PROMETHEUS",
        "type": "datasource",
        "query": "prometheus",
        "current": {},
        "hide": 0
      }
    ]
  },
  "panels": [
    {
      "title": "Total LLM Cost (USD)",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 0, "y": 0 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "unit": "currencyUSD",
          "thresholds": {
            "steps": [
              { "color": "green", "value": null },
              { "color": "yellow", "value": 1 },
              { "color": "red", "value": 10 }
            ]
          }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "sum(llm_cost_usd_total)",
          "legendFormat": "Total Cost",
          "refId": "A"
        }
      ],
      "options": { "reduceOptions": { "calcs": ["lastNotNull"] } }
    },
    {
      "title": "Total LLM Calls",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 6, "y": 0 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "thresholds": {
            "steps": [{ "color": "blue", "value": null }]
          }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "sum(llm_calls_total)",
          "legendFormat": "Calls",
          "refId": "A"
        }
      ],
      "options": { "reduceOptions": { "calcs": ["lastNotNull"] } }
    },
    {
      "title": "Total Input Tokens",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 12, "y": 0 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "thresholds": {
            "steps": [{ "color": "purple", "value": null }]
          }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "sum(gen_ai_usage_input_tokens_total)",
          "legendFormat": "Input Tokens",
          "refId": "A"
        }
      ],
      "options": { "reduceOptions": { "calcs": ["lastNotNull"] } }
    },
    {
      "title": "Total Output Tokens",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 18, "y": 0 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "thresholds": {
            "steps": [{ "color": "orange", "value": null }]
          }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "sum(gen_ai_usage_output_tokens_total)",
          "legendFormat": "Output Tokens",
          "refId": "A"
        }
      ],
      "options": { "reduceOptions": { "calcs": ["lastNotNull"] } }
    },
    {
      "title": "LLM Cost Over Time (by Model)",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 4 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "unit": "currencyUSD",
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 20,
            "stacking": { "mode": "normal" }
          }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "sum by (gen_ai_request_model) (increase(llm_cost_usd_total[5m]))",
          "legendFormat": "{{gen_ai_request_model}}",
          "refId": "A"
        }
      ]
    },
    {
      "title": "Token Usage Over Time (Input vs Output)",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 12, "y": 4 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 15,
            "stacking": { "mode": "normal" }
          }
        },
        "overrides": [
          {
            "matcher": { "id": "byRegexp", "options": "Input.*" },
            "properties": [{ "id": "color", "value": { "fixedColor": "blue", "mode": "fixed" } }]
          },
          {
            "matcher": { "id": "byRegexp", "options": "Output.*" },
            "properties": [{ "id": "color", "value": { "fixedColor": "orange", "mode": "fixed" } }]
          }
        ]
      },
      "targets": [
        {
          "expr": "sum by (gen_ai_request_model) (increase(gen_ai_usage_input_tokens_total[5m]))",
          "legendFormat": "Input - {{gen_ai_request_model}}",
          "refId": "A"
        },
        {
          "expr": "sum by (gen_ai_request_model) (increase(gen_ai_usage_output_tokens_total[5m]))",
          "legendFormat": "Output - {{gen_ai_request_model}}",
          "refId": "B"
        }
      ]
    },
    {
      "title": "LLM Call Latency Percentiles",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 12 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "unit": "ms",
          "custom": { "drawStyle": "line", "fillOpacity": 5 }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.50, sum(rate(llm_latency_milliseconds_bucket[5m])) by (le))",
          "legendFormat": "p50",
          "refId": "A"
        },
        {
          "expr": "histogram_quantile(0.95, sum(rate(llm_latency_milliseconds_bucket[5m])) by (le))",
          "legendFormat": "p95",
          "refId": "B"
        },
        {
          "expr": "histogram_quantile(0.99, sum(rate(llm_latency_milliseconds_bucket[5m])) by (le))",
          "legendFormat": "p99",
          "refId": "C"
        }
      ]
    },
    {
      "title": "LLM Calls by Model",
      "type": "piechart",
      "gridPos": { "h": 8, "w": 6, "x": 12, "y": 12 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "targets": [
        {
          "expr": "sum by (gen_ai_request_model) (llm_calls_total)",
          "legendFormat": "{{gen_ai_request_model}}",
          "refId": "A",
          "instant": true
        }
      ],
      "options": {
        "legend": { "displayMode": "table", "placement": "bottom" },
        "pieType": "donut"
      }
    },
    {
      "title": "LLM Calls by Provider",
      "type": "piechart",
      "gridPos": { "h": 8, "w": 6, "x": 18, "y": 12 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "targets": [
        {
          "expr": "sum by (gen_ai_system) (llm_calls_total)",
          "legendFormat": "{{gen_ai_system}}",
          "refId": "A",
          "instant": true
        }
      ],
      "options": {
        "legend": { "displayMode": "table", "placement": "bottom" },
        "pieType": "donut"
      }
    },
    {
      "title": "LLM Latency by Model",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 24, "x": 0, "y": 20 },
      "datasource": { "uid": "${DS_PROMETHEUS}", "type": "prometheus" },
      "fieldConfig": {
        "defaults": {
          "unit": "ms",
          "custom": { "drawStyle": "line", "fillOpacity": 10 }
        },
        "overrides": []
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.50, sum by (le, gen_ai_request_model) (rate(llm_latency_milliseconds_bucket[5m])))",
          "legendFormat": "p50 - {{gen_ai_request_model}}",
          "refId": "A"
        },
        {
          "expr": "histogram_quantile(0.95, sum by (le, gen_ai_request_model) (rate(llm_latency_milliseconds_bucket[5m])))",
          "legendFormat": "p95 - {{gen_ai_request_model}}",
          "refId": "B"
        }
      ]
    }
  ],
  "schemaVersion": 39,
  "version": 1
}
